PREHOOK: query: create table src_100 (key string, value string)
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@src_100
POSTHOOK: query: create table src_100 (key string, value string)
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@src_100
PREHOOK: query: insert into table src_100 select * from src limit 100
PREHOOK: type: QUERY
PREHOOK: Input: default@src
PREHOOK: Output: default@src_100
POSTHOOK: query: insert into table src_100 select * from src limit 100
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
POSTHOOK: Output: default@src_100
POSTHOOK: Lineage: src_100.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: src_100.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
PREHOOK: query: explain extended select key,value from src_100 order by key
PREHOOK: type: QUERY
POSTHOOK: query: explain extended select key,value from src_100 order by key
POSTHOOK: type: QUERY
ABSTRACT SYNTAX TREE:
  
TOK_QUERY
   TOK_FROM
      TOK_TABREF
         TOK_TABNAME
            src_100
   TOK_INSERT
      TOK_DESTINATION
         TOK_DIR
            TOK_TMP_FILE
      TOK_SELECT
         TOK_SELEXPR
            TOK_TABLE_OR_COL
               key
         TOK_SELEXPR
            TOK_TABLE_OR_COL
               value
      TOK_ORDERBY
         TOK_TABSORTCOLNAMEASC
            TOK_TABLE_OR_COL
               key


STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: src_100
            Statistics: Num rows: 100 Data size: 1076 Basic stats: COMPLETE Column stats: NONE
            GatherStats: false
            Select Operator
              expressions: key (type: string), value (type: string)
              outputColumnNames: _col0, _col1
              Statistics: Num rows: 100 Data size: 1076 Basic stats: COMPLETE Column stats: NONE
              Reduce Output Operator
                key expressions: _col0 (type: string)
                sort order: +
                Statistics: Num rows: 100 Data size: 1076 Basic stats: COMPLETE Column stats: NONE
                tag: -1
                value expressions: _col1 (type: string)
                auto parallelism: false
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: src_100
            input format: org.apache.hadoop.mapred.TextInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
            properties:
              COLUMN_STATS_ACCURATE true
              bucket_count -1
              columns key,value
              columns.comments 
              columns.types string:string
#### A masked pattern was here ####
              name default.src_100
              numFiles 1
              numRows 100
              rawDataSize 1076
              serialization.ddl struct src_100 { string key, string value}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              totalSize 1176
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                COLUMN_STATS_ACCURATE true
                bucket_count -1
                columns key,value
                columns.comments 
                columns.types string:string
#### A masked pattern was here ####
                name default.src_100
                numFiles 1
                numRows 100
                rawDataSize 1076
                serialization.ddl struct src_100 { string key, string value}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                totalSize 1176
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.src_100
            name: default.src_100
      Truncated Path -> Alias:
        /src_100 [src_100]
      Needs Tagging: false
      Reduce Operator Tree:
        Select Operator
          expressions: KEY.reducesinkkey0 (type: string), VALUE._col0 (type: string)
          outputColumnNames: _col0, _col1
          Statistics: Num rows: 100 Data size: 1076 Basic stats: COMPLETE Column stats: NONE
          File Output Operator
            compressed: false
            GlobalTableId: 0
#### A masked pattern was here ####
            NumFilesPerFileSink: 1
            Statistics: Num rows: 100 Data size: 1076 Basic stats: COMPLETE Column stats: NONE
#### A masked pattern was here ####
            table:
                input format: org.apache.hadoop.mapred.TextInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                properties:
                  columns _col0,_col1
                  columns.types string:string
                  escape.delim \
                  hive.serialization.extend.nesting.levels true
                  serialization.format 1
                  serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            TotalFiles: 1
            GatherStats: false
            MultiFileSpray: false

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Merge Keys: _col0 (type: string)
      Processor Tree:
        ListSink

PREHOOK: query: select key,value from src_100 order by key
PREHOOK: type: QUERY
PREHOOK: Input: default@src_100
#### A masked pattern was here ####
POSTHOOK: query: select key,value from src_100 order by key
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src_100
#### A masked pattern was here ####
0	val_0
113	val_113
128	val_128
128	val_128
129	val_129
145	val_145
146	val_146
149	val_149
15	val_15
150	val_150
152	val_152
153	val_153
155	val_155
157	val_157
162	val_162
165	val_165
166	val_166
167	val_167
17	val_17
170	val_170
174	val_174
174	val_174
193	val_193
193	val_193
195	val_195
199	val_199
20	val_20
203	val_203
205	val_205
207	val_207
208	val_208
209	val_209
213	val_213
219	val_219
221	val_221
224	val_224
237	val_237
238	val_238
247	val_247
252	val_252
255	val_255
265	val_265
266	val_266
27	val_27
273	val_273
277	val_277
278	val_278
281	val_281
287	val_287
292	val_292
302	val_302
309	val_309
311	val_311
311	val_311
316	val_316
325	val_325
327	val_327
338	val_338
339	val_339
342	val_342
345	val_345
365	val_365
367	val_367
369	val_369
37	val_37
374	val_374
377	val_377
378	val_378
394	val_394
396	val_396
397	val_397
399	val_399
401	val_401
403	val_403
406	val_406
409	val_409
413	val_413
417	val_417
417	val_417
429	val_429
430	val_430
438	val_438
439	val_439
446	val_446
455	val_455
459	val_459
466	val_466
469	val_469
475	val_475
482	val_482
484	val_484
489	val_489
489	val_489
494	val_494
495	val_495
57	val_57
66	val_66
82	val_82
86	val_86
98	val_98
PREHOOK: query: explain extended select sum(key) as sum, value from src_100 group by value order by sum
PREHOOK: type: QUERY
POSTHOOK: query: explain extended select sum(key) as sum, value from src_100 group by value order by sum
POSTHOOK: type: QUERY
ABSTRACT SYNTAX TREE:
  
TOK_QUERY
   TOK_FROM
      TOK_TABREF
         TOK_TABNAME
            src_100
   TOK_INSERT
      TOK_DESTINATION
         TOK_DIR
            TOK_TMP_FILE
      TOK_SELECT
         TOK_SELEXPR
            TOK_FUNCTION
               sum
               TOK_TABLE_OR_COL
                  key
            sum
         TOK_SELEXPR
            TOK_TABLE_OR_COL
               value
      TOK_GROUPBY
         TOK_TABLE_OR_COL
            value
      TOK_ORDERBY
         TOK_TABSORTCOLNAMEASC
            TOK_TABLE_OR_COL
               sum


STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-2 depends on stages: Stage-1
  Stage-0 depends on stages: Stage-2

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: src_100
            Statistics: Num rows: 100 Data size: 1076 Basic stats: COMPLETE Column stats: NONE
            GatherStats: false
            Select Operator
              expressions: value (type: string), key (type: string)
              outputColumnNames: _col0, _col1
              Statistics: Num rows: 100 Data size: 1076 Basic stats: COMPLETE Column stats: NONE
              Group By Operator
                aggregations: sum(_col1)
                keys: _col0 (type: string)
                mode: hash
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 100 Data size: 1076 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  key expressions: _col0 (type: string)
                  sort order: +
                  Map-reduce partition columns: _col0 (type: string)
                  Statistics: Num rows: 100 Data size: 1076 Basic stats: COMPLETE Column stats: NONE
                  tag: -1
                  value expressions: _col1 (type: double)
                  auto parallelism: false
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: src_100
            input format: org.apache.hadoop.mapred.TextInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
            properties:
              COLUMN_STATS_ACCURATE true
              bucket_count -1
              columns key,value
              columns.comments 
              columns.types string:string
#### A masked pattern was here ####
              name default.src_100
              numFiles 1
              numRows 100
              rawDataSize 1076
              serialization.ddl struct src_100 { string key, string value}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              totalSize 1176
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                COLUMN_STATS_ACCURATE true
                bucket_count -1
                columns key,value
                columns.comments 
                columns.types string:string
#### A masked pattern was here ####
                name default.src_100
                numFiles 1
                numRows 100
                rawDataSize 1076
                serialization.ddl struct src_100 { string key, string value}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                totalSize 1176
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.src_100
            name: default.src_100
      Truncated Path -> Alias:
        /src_100 [$hdt$_0:$hdt$_0:src_100]
      Needs Tagging: false
      Reduce Operator Tree:
        Group By Operator
          aggregations: sum(VALUE._col0)
          keys: KEY._col0 (type: string)
          mode: mergepartial
          outputColumnNames: _col0, _col1
          Statistics: Num rows: 50 Data size: 538 Basic stats: COMPLETE Column stats: NONE
          Select Operator
            expressions: _col1 (type: double), _col0 (type: string)
            outputColumnNames: _col0, _col1
            Statistics: Num rows: 50 Data size: 538 Basic stats: COMPLETE Column stats: NONE
            File Output Operator
              compressed: false
              GlobalTableId: 0
#### A masked pattern was here ####
              NumFilesPerFileSink: 1
              table:
                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                  properties:
                    columns _col0,_col1
                    columns.types double,string
                    escape.delim \
                    serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
                  serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
              TotalFiles: 1
              GatherStats: false
              MultiFileSpray: false

  Stage: Stage-2
    Map Reduce
      Map Operator Tree:
          TableScan
            GatherStats: false
            Reduce Output Operator
              key expressions: _col0 (type: double)
              sort order: +
              Statistics: Num rows: 50 Data size: 538 Basic stats: COMPLETE Column stats: NONE
              tag: -1
              value expressions: _col1 (type: string)
              auto parallelism: false
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: -mr-10003
            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
            properties:
              columns _col0,_col1
              columns.types double,string
              escape.delim \
              serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
          
              input format: org.apache.hadoop.mapred.SequenceFileInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
              properties:
                columns _col0,_col1
                columns.types double,string
                escape.delim \
                serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
              serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
      Truncated Path -> Alias:
#### A masked pattern was here ####
      Needs Tagging: false
      Reduce Operator Tree:
        Select Operator
          expressions: KEY.reducesinkkey0 (type: double), VALUE._col0 (type: string)
          outputColumnNames: _col0, _col1
          Statistics: Num rows: 50 Data size: 538 Basic stats: COMPLETE Column stats: NONE
          File Output Operator
            compressed: false
            GlobalTableId: 0
#### A masked pattern was here ####
            NumFilesPerFileSink: 1
            Statistics: Num rows: 50 Data size: 538 Basic stats: COMPLETE Column stats: NONE
#### A masked pattern was here ####
            table:
                input format: org.apache.hadoop.mapred.TextInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                properties:
                  columns _col0,_col1
                  columns.types double:string
                  escape.delim \
                  hive.serialization.extend.nesting.levels true
                  serialization.format 1
                  serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            TotalFiles: 1
            GatherStats: false
            MultiFileSpray: false

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Merge Keys: _col0 (type: double)
      Processor Tree:
        ListSink

PREHOOK: query: select sum(key) as sum, value from src_100 group by value order by sum
PREHOOK: type: QUERY
PREHOOK: Input: default@src_100
#### A masked pattern was here ####
POSTHOOK: query: select sum(key) as sum, value from src_100 group by value order by sum
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src_100
#### A masked pattern was here ####
0.0	val_0
15.0	val_15
17.0	val_17
20.0	val_20
27.0	val_27
37.0	val_37
57.0	val_57
66.0	val_66
82.0	val_82
86.0	val_86
98.0	val_98
113.0	val_113
129.0	val_129
145.0	val_145
146.0	val_146
149.0	val_149
150.0	val_150
152.0	val_152
153.0	val_153
155.0	val_155
157.0	val_157
162.0	val_162
165.0	val_165
166.0	val_166
167.0	val_167
170.0	val_170
195.0	val_195
199.0	val_199
203.0	val_203
205.0	val_205
207.0	val_207
208.0	val_208
209.0	val_209
213.0	val_213
219.0	val_219
221.0	val_221
224.0	val_224
237.0	val_237
238.0	val_238
247.0	val_247
252.0	val_252
255.0	val_255
256.0	val_128
265.0	val_265
266.0	val_266
273.0	val_273
277.0	val_277
278.0	val_278
281.0	val_281
287.0	val_287
292.0	val_292
302.0	val_302
309.0	val_309
316.0	val_316
325.0	val_325
327.0	val_327
338.0	val_338
339.0	val_339
342.0	val_342
345.0	val_345
348.0	val_174
365.0	val_365
367.0	val_367
369.0	val_369
374.0	val_374
377.0	val_377
378.0	val_378
386.0	val_193
394.0	val_394
396.0	val_396
397.0	val_397
399.0	val_399
401.0	val_401
403.0	val_403
406.0	val_406
409.0	val_409
413.0	val_413
429.0	val_429
430.0	val_430
438.0	val_438
439.0	val_439
446.0	val_446
455.0	val_455
459.0	val_459
466.0	val_466
469.0	val_469
475.0	val_475
482.0	val_482
484.0	val_484
494.0	val_494
495.0	val_495
622.0	val_311
834.0	val_417
978.0	val_489
PREHOOK: query: -- negative, subquery
explain extended select sum(key), a.value from (select * from src_100 order by key) a group by a.value
PREHOOK: type: QUERY
POSTHOOK: query: -- negative, subquery
explain extended select sum(key), a.value from (select * from src_100 order by key) a group by a.value
POSTHOOK: type: QUERY
ABSTRACT SYNTAX TREE:
  
TOK_QUERY
   TOK_FROM
      TOK_SUBQUERY
         TOK_QUERY
            TOK_FROM
               TOK_TABREF
                  TOK_TABNAME
                     src_100
            TOK_INSERT
               TOK_DESTINATION
                  TOK_DIR
                     TOK_TMP_FILE
               TOK_SELECT
                  TOK_SELEXPR
                     TOK_ALLCOLREF
               TOK_ORDERBY
                  TOK_TABSORTCOLNAMEASC
                     TOK_TABLE_OR_COL
                        key
         a
   TOK_INSERT
      TOK_DESTINATION
         TOK_DIR
            TOK_TMP_FILE
      TOK_SELECT
         TOK_SELEXPR
            TOK_FUNCTION
               sum
               TOK_TABLE_OR_COL
                  key
         TOK_SELEXPR
            .
               TOK_TABLE_OR_COL
                  a
               value
      TOK_GROUPBY
         .
            TOK_TABLE_OR_COL
               a
            value


STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-2 depends on stages: Stage-1
  Stage-0 depends on stages: Stage-2

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: src_100
            Statistics: Num rows: 100 Data size: 1076 Basic stats: COMPLETE Column stats: NONE
            GatherStats: false
            Select Operator
              expressions: key (type: string), value (type: string)
              outputColumnNames: _col0, _col1
              Statistics: Num rows: 100 Data size: 1076 Basic stats: COMPLETE Column stats: NONE
              Reduce Output Operator
                key expressions: _col0 (type: string)
                sort order: +
                Statistics: Num rows: 100 Data size: 1076 Basic stats: COMPLETE Column stats: NONE
                tag: -1
                value expressions: _col1 (type: string)
                auto parallelism: false
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: src_100
            input format: org.apache.hadoop.mapred.TextInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
            properties:
              COLUMN_STATS_ACCURATE true
              bucket_count -1
              columns key,value
              columns.comments 
              columns.types string:string
#### A masked pattern was here ####
              name default.src_100
              numFiles 1
              numRows 100
              rawDataSize 1076
              serialization.ddl struct src_100 { string key, string value}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              totalSize 1176
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                COLUMN_STATS_ACCURATE true
                bucket_count -1
                columns key,value
                columns.comments 
                columns.types string:string
#### A masked pattern was here ####
                name default.src_100
                numFiles 1
                numRows 100
                rawDataSize 1076
                serialization.ddl struct src_100 { string key, string value}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                totalSize 1176
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.src_100
            name: default.src_100
      Truncated Path -> Alias:
        /src_100 [$hdt$_0:$hdt$_0:src_100]
      Needs Tagging: false
      Reduce Operator Tree:
        Select Operator
          expressions: VALUE._col0 (type: string), KEY.reducesinkkey0 (type: string)
          outputColumnNames: _col0, _col1
          Statistics: Num rows: 100 Data size: 1076 Basic stats: COMPLETE Column stats: NONE
          Group By Operator
            aggregations: sum(_col1)
            keys: _col0 (type: string)
            mode: hash
            outputColumnNames: _col0, _col1
            Statistics: Num rows: 100 Data size: 1076 Basic stats: COMPLETE Column stats: NONE
            File Output Operator
              compressed: false
              GlobalTableId: 0
#### A masked pattern was here ####
              NumFilesPerFileSink: 1
              table:
                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                  properties:
                    columns _col0,_col1
                    columns.types string,double
                    escape.delim \
                    serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
                  serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
              TotalFiles: 1
              GatherStats: false
              MultiFileSpray: false

  Stage: Stage-2
    Map Reduce
      Map Operator Tree:
          TableScan
            GatherStats: false
            Reduce Output Operator
              key expressions: _col0 (type: string)
              sort order: +
              Map-reduce partition columns: _col0 (type: string)
              Statistics: Num rows: 100 Data size: 1076 Basic stats: COMPLETE Column stats: NONE
              tag: -1
              value expressions: _col1 (type: double)
              auto parallelism: false
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: -mr-10003
            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
            properties:
              columns _col0,_col1
              columns.types string,double
              escape.delim \
              serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
          
              input format: org.apache.hadoop.mapred.SequenceFileInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
              properties:
                columns _col0,_col1
                columns.types string,double
                escape.delim \
                serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
              serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
      Truncated Path -> Alias:
#### A masked pattern was here ####
      Needs Tagging: false
      Reduce Operator Tree:
        Group By Operator
          aggregations: sum(VALUE._col0)
          keys: KEY._col0 (type: string)
          mode: mergepartial
          outputColumnNames: _col0, _col1
          Statistics: Num rows: 50 Data size: 538 Basic stats: COMPLETE Column stats: NONE
          Select Operator
            expressions: _col1 (type: double), _col0 (type: string)
            outputColumnNames: _col0, _col1
            Statistics: Num rows: 50 Data size: 538 Basic stats: COMPLETE Column stats: NONE
            File Output Operator
              compressed: false
              GlobalTableId: 0
#### A masked pattern was here ####
              NumFilesPerFileSink: 1
              Statistics: Num rows: 50 Data size: 538 Basic stats: COMPLETE Column stats: NONE
#### A masked pattern was here ####
              table:
                  input format: org.apache.hadoop.mapred.TextInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                  properties:
                    columns _col0,_col1
                    columns.types double:string
                    escape.delim \
                    hive.serialization.extend.nesting.levels true
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              TotalFiles: 1
              GatherStats: false
              MultiFileSpray: false

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: -- negative, insert
CREATE TABLE insert_temp (key int, value string)
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@insert_temp
POSTHOOK: query: -- negative, insert
CREATE TABLE insert_temp (key int, value string)
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@insert_temp
PREHOOK: query: EXPLAIN extended INSERT INTO TABLE insert_temp SELECT sum(key) as sum, value from src_100 group by value order by sum
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN extended INSERT INTO TABLE insert_temp SELECT sum(key) as sum, value from src_100 group by value order by sum
POSTHOOK: type: QUERY
ABSTRACT SYNTAX TREE:
  
TOK_QUERY
   TOK_FROM
      TOK_TABREF
         TOK_TABNAME
            src_100
   TOK_INSERT
      TOK_INSERT_INTO
         TOK_TAB
            TOK_TABNAME
               insert_temp
      TOK_SELECT
         TOK_SELEXPR
            TOK_FUNCTION
               sum
               TOK_TABLE_OR_COL
                  key
            sum
         TOK_SELEXPR
            TOK_TABLE_OR_COL
               value
      TOK_GROUPBY
         TOK_TABLE_OR_COL
            value
      TOK_ORDERBY
         TOK_TABSORTCOLNAMEASC
            TOK_TABLE_OR_COL
               sum


STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-2 depends on stages: Stage-1
  Stage-0 depends on stages: Stage-2
  Stage-3 depends on stages: Stage-0

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: src_100
            Statistics: Num rows: 100 Data size: 1076 Basic stats: COMPLETE Column stats: NONE
            GatherStats: false
            Select Operator
              expressions: value (type: string), key (type: string)
              outputColumnNames: _col0, _col1
              Statistics: Num rows: 100 Data size: 1076 Basic stats: COMPLETE Column stats: NONE
              Group By Operator
                aggregations: sum(_col1)
                keys: _col0 (type: string)
                mode: hash
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 100 Data size: 1076 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  key expressions: _col0 (type: string)
                  sort order: +
                  Map-reduce partition columns: _col0 (type: string)
                  Statistics: Num rows: 100 Data size: 1076 Basic stats: COMPLETE Column stats: NONE
                  tag: -1
                  value expressions: _col1 (type: double)
                  auto parallelism: false
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: src_100
            input format: org.apache.hadoop.mapred.TextInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
            properties:
              COLUMN_STATS_ACCURATE true
              bucket_count -1
              columns key,value
              columns.comments 
              columns.types string:string
#### A masked pattern was here ####
              name default.src_100
              numFiles 1
              numRows 100
              rawDataSize 1076
              serialization.ddl struct src_100 { string key, string value}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              totalSize 1176
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                COLUMN_STATS_ACCURATE true
                bucket_count -1
                columns key,value
                columns.comments 
                columns.types string:string
#### A masked pattern was here ####
                name default.src_100
                numFiles 1
                numRows 100
                rawDataSize 1076
                serialization.ddl struct src_100 { string key, string value}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                totalSize 1176
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.src_100
            name: default.src_100
      Truncated Path -> Alias:
        /src_100 [$hdt$_0:$hdt$_0:src_100]
      Needs Tagging: false
      Reduce Operator Tree:
        Group By Operator
          aggregations: sum(VALUE._col0)
          keys: KEY._col0 (type: string)
          mode: mergepartial
          outputColumnNames: _col0, _col1
          Statistics: Num rows: 50 Data size: 538 Basic stats: COMPLETE Column stats: NONE
          Select Operator
            expressions: _col1 (type: double), _col0 (type: string)
            outputColumnNames: _col0, _col1
            Statistics: Num rows: 50 Data size: 538 Basic stats: COMPLETE Column stats: NONE
            File Output Operator
              compressed: false
              GlobalTableId: 0
#### A masked pattern was here ####
              NumFilesPerFileSink: 1
              table:
                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                  properties:
                    columns _col0,_col1
                    columns.types double,string
                    escape.delim \
                    serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
                  serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
              TotalFiles: 1
              GatherStats: false
              MultiFileSpray: false

  Stage: Stage-2
    Map Reduce
      Map Operator Tree:
          TableScan
            GatherStats: false
            Reduce Output Operator
              key expressions: _col0 (type: double)
              sort order: +
              Statistics: Num rows: 50 Data size: 538 Basic stats: COMPLETE Column stats: NONE
              tag: -1
              value expressions: _col1 (type: string)
              auto parallelism: false
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: -mr-10001
            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
            properties:
              columns _col0,_col1
              columns.types double,string
              escape.delim \
              serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
          
              input format: org.apache.hadoop.mapred.SequenceFileInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
              properties:
                columns _col0,_col1
                columns.types double,string
                escape.delim \
                serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
              serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
      Truncated Path -> Alias:
#### A masked pattern was here ####
      Needs Tagging: false
      Reduce Operator Tree:
        Select Operator
          expressions: UDFToInteger(KEY.reducesinkkey0) (type: int), VALUE._col0 (type: string)
          outputColumnNames: _col0, _col1
          Statistics: Num rows: 50 Data size: 538 Basic stats: COMPLETE Column stats: NONE
          File Output Operator
            compressed: false
            GlobalTableId: 1
#### A masked pattern was here ####
            NumFilesPerFileSink: 1
            Statistics: Num rows: 50 Data size: 538 Basic stats: COMPLETE Column stats: NONE
#### A masked pattern was here ####
            table:
                input format: org.apache.hadoop.mapred.TextInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                properties:
                  bucket_count -1
                  columns key,value
                  columns.comments 
                  columns.types int:string
#### A masked pattern was here ####
                  name default.insert_temp
                  serialization.ddl struct insert_temp { i32 key, string value}
                  serialization.format 1
                  serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
#### A masked pattern was here ####
                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                name: default.insert_temp
            TotalFiles: 1
            GatherStats: true
            MultiFileSpray: false

  Stage: Stage-0
    Move Operator
      tables:
          replace: false
#### A masked pattern was here ####
          table:
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                bucket_count -1
                columns key,value
                columns.comments 
                columns.types int:string
#### A masked pattern was here ####
                name default.insert_temp
                serialization.ddl struct insert_temp { i32 key, string value}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.insert_temp

  Stage: Stage-3
    Stats-Aggr Operator
#### A masked pattern was here ####

